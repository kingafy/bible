{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8- Sarcasm Classifiers (GloVe and CNN).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LXk6D0Kp-n5s",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BhwmXQKo-kRw",
        "colab": {}
      },
      "source": [
        "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv -P /tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0fRashl-kR1",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "packages = [\n",
        "    'JohnSnowLabs:spark-nlp: 2.5.5'\n",
        "]\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"ML SQL session\") \\\n",
        "    .config('spark.jars.packages', ','.join(packages)) \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nAQyYg5m-kR4",
        "colab": {}
      },
      "source": [
        "import sparknlp\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJpTEyT7-kR7",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sql = SQLContext(spark)\n",
        "\n",
        "trainBalancedSarcasmDF = spark.read.option(\"header\", True).option(\"inferSchema\", True) \\\n",
        "    .csv(\"/tmp/train-balanced-sarcasm.csv\")\n",
        "trainBalancedSarcasmDF.printSchema()\n",
        "\n",
        "# Let's create a temp view (table) for our SQL queries\n",
        "trainBalancedSarcasmDF.createOrReplaceTempView('sarcasm')\n",
        "\n",
        "sql.sql('SELECT COUNT(*) FROM sarcasm').collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BlTyHWxX-kR9",
        "colab": {}
      },
      "source": [
        "df = sql.sql('''\n",
        "select label, concat(parent_comment,\"\\n\",comment) as comment \n",
        "from sarcasm \n",
        "where comment is not null and parent_comment is not null limit 10000''')\n",
        "print(type(df))\n",
        "df.printSchema()\n",
        "print('rows', df.count())\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sTtgT0nl-kSA",
        "colab": {}
      },
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"comment\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\") \\\n",
        "    .setUseAbbreviations(True)\n",
        "    \n",
        "tokenizer = Tokenizer() \\\n",
        "  .setInputCols([\"sentence\"]) \\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer])\n",
        "nlp_model = nlp_pipeline.fit(df)\n",
        "\n",
        "processed = nlp_model.transform(df)\n",
        "processed.show()\n",
        "\n",
        "train, test = processed.randomSplit(weights=[0.7, 0.3], seed=123)\n",
        "\n",
        "print(train.count())\n",
        "print(test.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Ja-TkfF-kSD",
        "colab": {}
      },
      "source": [
        "glove = WordEmbeddingsModel.pretrained()\n",
        "train_featurized = glove.transform(train)\n",
        "train_featurized.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lKWVvLXm-kSF",
        "colab": {}
      },
      "source": [
        "test_featurized = glove.transform(test)\n",
        "test_featurized.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y3Zpptsh-kSI",
        "colab": {}
      },
      "source": [
        "def get_features(row):\n",
        "    result = []\n",
        "    for tk in row:\n",
        "        result.append(tk['embeddings'])\n",
        "    return np.array(result)\n",
        "\n",
        "def build_data(df, chunks=10):\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "\n",
        "    row_count = df.count()\n",
        "    i = 0\n",
        "    \n",
        "    chunks = df.randomSplit(weights=[1/chunks] * chunks)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        rows = chunk.collect()\n",
        "        for row in rows:\n",
        "            if i % 1000 == 0:\n",
        "                print('row {} / {} ({:.1f} %)'.format(i, row_count, 100 * i / row_count))\n",
        "            embeddings = get_features(row['embeddings'])\n",
        "            label = row['label']\n",
        "            x_train.append(embeddings)\n",
        "            y_train.append(label)\n",
        "            i += 1\n",
        "\n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "    return x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDcDax6R-kSK",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "x_train, y_train = build_data(train_featurized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pHzesw6c-kSN",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "x_test, y_test = build_data(test_featurized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EkrSsFu8-kSP",
        "colab": {}
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bx7ybKOy-kSS",
        "colab": {}
      },
      "source": [
        "print('Train Labels:\\n', pd.Series(y_train).value_counts())\n",
        "print('Test Labels:\\n', pd.Series(y_test).value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WSOCSyhV-kSU",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# set parameters for our model:\n",
        "maxlen = 100 #max 50 words per article\n",
        "batch_size = 32 #size of the batch \n",
        "filters = 50 #dimension of filters for the convolutional layer\n",
        "kernel_size = 3 #size of the kernel used in the convolutional layer\n",
        "hidden_dims = 250 #dimension of the hidden layer\n",
        "epochs = 5 #number of training epochs\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SFEyL-WY-kSX",
        "colab": {}
      },
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy','mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ulTZGHRI-kSa",
        "colab": {}
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VgGva_h--kSc",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "dot = model_to_dot(model)\n",
        "Image(dot.create_png())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
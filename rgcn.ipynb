{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rgcn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUJCrXRIj0zN",
        "colab_type": "text"
      },
      "source": [
        "# R-GCN in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSe5Pmn6pbdu",
        "colab_type": "text"
      },
      "source": [
        "In this notebook I describe how to develop Relational Graph Convolutional Networks (R-GCNs) of the link-prediction task within Knowledge Graphs. For more information on R-GCNs we suggest the following article:\n",
        "\n",
        "\n",
        "*   Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., & Welling, M. (2018, June). [Modeling relational data with graph convolutional networks](https://link.springer.com/chapter/10.1007/978-3-319-93417-4_38). In European Semantic Web Conference (pp. 593-607). Springer, Cham.\n",
        "\n",
        "I take some parts of this article in order to describe and explain the source code deployed in this notebook. The main part of this code is inspired by the Deep Graph Library (DGL - https://www.dgl.ai/), that is a python package built to ease deep learning on graph, on top of existing deep learning frameworks. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjGtOjY8cD73",
        "colab_type": "text"
      },
      "source": [
        "# Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfbL9l93cI49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install dgl\n",
        "!pip install rdflib\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYO-zCwXgw1W",
        "colab_type": "text"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjAD4fq_cck8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available(): # Try to use GPU if available\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "print('Your device is ' + str(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgkie2ZaOemx",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QceuLDaMSwti",
        "colab_type": "text"
      },
      "source": [
        "Most code of the Utils section is adapted from authors' implementation of RGCN link prediction:\n",
        "https://github.com/MichSchli/RelationPrediction.\n",
        "\n",
        "In the utils section are implemented functions that are useful for the following steps:\n",
        "\n",
        "\n",
        "1.   Compute the adjacency matrix of the knowledge graph and the degrees of each node.\n",
        "2.   Reduce the training time sampling the edge neighborhood.\n",
        "3.   Perform the negative sampling to further reduce the training time.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFXMJbFEO0Wm",
        "colab_type": "text"
      },
      "source": [
        "## Functions for Graph Sampling and Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz6DvX1qOeM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_adj_and_degrees(num_nodes, triplets):\n",
        "    \"\"\" \n",
        "    Get adjacency list and degrees of the graph\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    num_nodes -- (int) number of nodes in the graph\n",
        "    triplets -- (matrix) size(number of training samples, 3). Each row of the\n",
        "                matrix is in the form: [subject_id, relation_id, object_id]\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    adj_list -- (list) size(number of nodes, node degree)\n",
        "    degrees -- (vector) size(, number of nodes). Each element of the vector is\n",
        "               filled with the degree of the node.\n",
        "    \"\"\"\n",
        "    adj_list = [[] for _ in range(num_nodes)]\n",
        "    for i,triplet in enumerate(triplets):\n",
        "        adj_list[triplet[0]].append([i, triplet[2]])\n",
        "        adj_list[triplet[2]].append([i, triplet[0]])\n",
        "\n",
        "    degrees = np.array([len(a) for a in adj_list])\n",
        "    adj_list = [np.array(a) for a in adj_list]\n",
        "    return adj_list, degrees\n",
        "\n",
        "  \n",
        "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
        "    \"\"\" \n",
        "    Edge neighborhood sampling to reduce graph size for training purposes\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    adj_list -- (structure) size(number of nodes, node degree)\n",
        "    degrees -- (vector) size(, number of nodes). Each element of the vector is\n",
        "               filled with the degree of the node\n",
        "    n_triplets -- (int) number of triples in the training data\n",
        "    sample_size -- (int) number of edges to sample in each iteration (parameter)\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    edges -- list of edge indexes as result of the sampling process\n",
        "    \"\"\"\n",
        "\n",
        "    edges = np.zeros((sample_size), dtype=np.int32)\n",
        "\n",
        "    # Initialization for sampling of nodes\n",
        "    sample_counts = np.array([d for d in degrees])\n",
        "    picked = np.array([False for _ in range(n_triplets)])\n",
        "    seen = np.array([False for _ in degrees])\n",
        "\n",
        "    for i in range(0, sample_size):\n",
        "        weights = sample_counts * seen\n",
        "\n",
        "        if np.sum(weights) == 0:\n",
        "            weights = np.ones_like(weights)\n",
        "            weights[np.where(sample_counts == 0)] = 0\n",
        "\n",
        "        probabilities = (weights) / np.sum(weights)\n",
        "        \n",
        "        # Choose vertex according to the computed probabilities\n",
        "        # The number of chosen vertices is based on the sample size parameter\n",
        "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
        "                                         p=probabilities)\n",
        "        \n",
        "        # Store all edges and nodes linked to the chosen vertex\n",
        "        chosen_adj_list = adj_list[chosen_vertex]\n",
        "        seen[chosen_vertex] = True\n",
        "        \n",
        "        # On the basis of the chosen vertex, randomly choose the edge and pick it up\n",
        "        # from the adjacency list and store its value in the edge number\n",
        "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
        "        chosen_edge = chosen_adj_list[chosen_edge]\n",
        "        edge_number = chosen_edge[0]\n",
        "        \n",
        "        # If the edge has been already picked, \n",
        "        # choose another one according to the same principles\n",
        "        while picked[edge_number]:\n",
        "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
        "            chosen_edge = chosen_adj_list[chosen_edge]\n",
        "            edge_number = chosen_edge[0]\n",
        "        \n",
        "        edges[i] = edge_number\n",
        "        other_vertex = chosen_edge[1]\n",
        "        picked[edge_number] = True\n",
        "        sample_counts[chosen_vertex] -= 1\n",
        "        sample_counts[other_vertex] -= 1\n",
        "        seen[other_vertex] = True\n",
        "        \n",
        "        # Return all edges from the sampling process\n",
        "    return edges\n",
        "\n",
        "  \n",
        "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
        "                                      num_rels, adj_list, degrees,\n",
        "                                      negative_rate):\n",
        "    \"\"\"\n",
        "    Get training graph and signals\n",
        "    First perform edge neighborhood sampling on graph, then perform negative\n",
        "    sampling to generate negative samples\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    triplets -- (matrix) size (num of triples in the training set, 3)\n",
        "    sample_size -- (int) number of nodes to take as samples\n",
        "    split_size -- (int) parameter to split the graph from (0 to 1)\n",
        "    num_rels -- (int) number of unique relations in the training set\n",
        "    adj_list -- (list) size(number of nodes, node degree)\n",
        "    degrees -- (vector) size(, number of nodes).\n",
        "    negative_rate -- (parameter)\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    g -- DGL graph\n",
        "    uniq_v -- (vector) unique nodes\n",
        "    rel -- (vector) relations\n",
        "    norm -- (vector) normalized degree values of each node\n",
        "    samples -- (matrix) triple samples considering positive and negative samples\n",
        "    labels -- (vector) size(,positive + negative samples). The value of each element\n",
        "              is 1 for positive samples and 0 for negative samples.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Perform edge neighbor sampling to take a subset of edges\n",
        "    edges = sample_edge_neighborhood(adj_list, degrees, len(triplets),\n",
        "                                     sample_size)\n",
        "\n",
        "    # Relabel nodes to have consecutive node ids\n",
        "    edges = triplets[edges] \n",
        "    src, rel, dst = edges.transpose()\n",
        "    # Get unique vertices: such vertices are the result of the sampling. \n",
        "    # At the end of this process, we obtain an array of new triplets:\n",
        "    # we use as node indexes the values of the array to reconstruct\n",
        "    # the original src and dst\n",
        "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
        "    src, dst = np.reshape(edges, (2, -1))\n",
        "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
        "\n",
        "    # Negative sampling\n",
        "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
        "                                        negative_rate)\n",
        "\n",
        "    # Further split graph, only half of the edges will be used as graph\n",
        "    # structure, while the rest half is used as unseen positive samples\n",
        "    # The graph is splitted according to split_size, picking up randomly\n",
        "    # values between 0 and sample_size. \n",
        "    # According to these random values, I create the new src, dst, and rel arrays\n",
        "    split_size = int(sample_size * split_size)\n",
        "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
        "                                       size=split_size, replace=False)\n",
        "    src = src[graph_split_ids]\n",
        "    dst = dst[graph_split_ids]\n",
        "    rel = rel[graph_split_ids]\n",
        "   \n",
        "    # Build DGL graph\n",
        "    # The number of nodes is equal to unique nodes after the sampling process\n",
        "    # The number of edges is equal to src*2, because we consider in and out edges\n",
        "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
        "    print(\"# sampled edges: {}\".format(len(rel) * 2))\n",
        "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
        "                                             (src, rel, dst))\n",
        "    \n",
        "    return g, uniq_v, rel, norm, samples, labels\n",
        "\n",
        "  \n",
        "def comp_deg_norm(g):\n",
        "    \"\"\"\n",
        "    Apply a normalization to the values of input degree for each node\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    g -- DGL Graph\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    norm -- np.array of normalized degree values - 1 X num of nodes\n",
        "    \"\"\"\n",
        "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
        "    norm = 1.0 / in_deg\n",
        "    # If the in_deg is 0, the result of normalization is inf (put 0 in this case)\n",
        "    norm[np.isinf(norm)] = 0\n",
        "\n",
        "    return norm\n",
        "\n",
        "  \n",
        "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
        "    \"\"\" \n",
        "    Create a DGL graph. The graph is bidirectional because RGCN authors\n",
        "    use reversed relations. This function also generates edge type and\n",
        "    normalization factor (reciprocal of node incoming degree)\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    num_nodes -- Number of nodes (result of the sampling)\n",
        "    num_rels -- Number of relations (doubled compared to the original ones)\n",
        "    triplets -- Three different arrays: src, rel, dst (result of the splitted graph)\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    g -- High-level representation of the graph created using the DGL library\n",
        "    rel -- (vector) vector of all relations\n",
        "    norm -- (vector) normalized-degree of nodes\n",
        "    \"\"\"\n",
        "    g = dgl.DGLGraph()\n",
        "    g.add_nodes(num_nodes)\n",
        "    src, rel, dst = triplets\n",
        "    \n",
        "    # The bidirectional graph is created (nodes are doubled)\n",
        "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
        "    \n",
        "    # The relations are doubled adding the number of different relations \n",
        "    rel = np.concatenate((rel, rel + num_rels))\n",
        "    \n",
        "    # Create the edges array\n",
        "    edges = sorted(zip(dst, src, rel))    \n",
        "    dst, src, rel = np.array(edges).transpose() # 3 x Num of nodes\n",
        "    g.add_edges(src, dst)\n",
        "    \n",
        "    norm = comp_deg_norm(g)\n",
        "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
        "    \n",
        "    return g, rel, norm\n",
        "\n",
        "  \n",
        "def build_test_graph(num_nodes, num_rels, edges):\n",
        "    src, rel, dst = edges.transpose()\n",
        "    print(\"Test graph:\")\n",
        "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
        "\n",
        "  \n",
        "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
        "    \"\"\"\n",
        "    Apply negative sampling\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    pos_samples -- Relabeled edges according to the sampling process\n",
        "    num_entity -- Number of entities\n",
        "    negative_rate -- Negative rate parameter (default 10)\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    np.array, labels -- np.array contains a concatenation of positive samples\n",
        "                        and negative samples. \n",
        "                        labels is an array where elements are equal to 1 for \n",
        "                        positive samples and 0 for negative samples\n",
        "    \"\"\"\n",
        "    size_of_batch = len(pos_samples)\n",
        "    num_to_generate = size_of_batch * negative_rate\n",
        "    \n",
        "    # Construct an array of negative samples by repeating \n",
        "    # the positive samples for the negative_rate value (np.tile).\n",
        "    # Then, I create a label array to perform the logistic regression related to\n",
        "    # the negative sampling. I initialize the first size_of_batch of labels to 1\n",
        "    # and the other values are equal to 0.\n",
        "    neg_samples = np.tile(pos_samples, (negative_rate, 1))        \n",
        "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
        "    labels[: size_of_batch] = 1\n",
        "    \n",
        "    # Create negative samples replacing the subject or the object according\n",
        "    # to the random probability generated in choices\n",
        "    # \n",
        "    # If the positive samples are the following: \n",
        "    # [[1540  154 2254]\n",
        "    # [ 510  193 1540]\n",
        "    # [1540   55 1269]...]\n",
        "    #\n",
        "    # The negative samples will be the following:\n",
        "    # [[1540  154 1750]\n",
        "    #  [1051  193 1540]\n",
        "    #  [85   55 1269]...]\n",
        "    #\n",
        "    # In other words we generate wrong triples\n",
        "    values = np.random.randint(num_entity, size=num_to_generate)\n",
        "    choices = np.random.uniform(size=num_to_generate)\n",
        "    subj = choices > 0.5\n",
        "    obj = choices <= 0.5\n",
        "    neg_samples[subj, 0] = values[subj]\n",
        "    neg_samples[obj, 2] = values[obj]\n",
        "    \n",
        "    return np.concatenate((pos_samples, neg_samples)), labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FG_eBC4Phk8",
        "colab_type": "text"
      },
      "source": [
        "## Functions for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hWSpJTKPiBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_and_rank(score, target):\n",
        "    _, indices = torch.sort(score, dim=1, descending=True)\n",
        "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
        "    indices = indices[:, 1].view(-1)\n",
        "    return indices\n",
        "\n",
        "  \n",
        "def perturb_and_get_rank(embedding, w, a, r, b, num_entity, batch_size=100):\n",
        "    \"\"\" \n",
        "    Perturb one element in the triplets\n",
        "    \n",
        "    \"\"\"\n",
        "    n_batch = (num_entity + batch_size - 1) // batch_size\n",
        "    ranks = []\n",
        "    for idx in range(n_batch):\n",
        "        print(\"batch {} / {}\".format(idx, n_batch))\n",
        "        batch_start = idx * batch_size\n",
        "        batch_end = min(num_entity, (idx + 1) * batch_size)\n",
        "        batch_a = a[batch_start: batch_end]\n",
        "        batch_r = r[batch_start: batch_end]\n",
        "        emb_ar = embedding[batch_a] * w[batch_r]\n",
        "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
        "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
        "        # out-prod and reduce sum\n",
        "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
        "        score = torch.sum(out_prod, dim=0) # size E x V\n",
        "        score = torch.sigmoid(score)\n",
        "        target = b[batch_start: batch_end]\n",
        "        ranks.append(sort_and_rank(score, target))\n",
        "    return torch.cat(ranks)\n",
        "\n",
        "\n",
        "# return MRR (raw), and Hits @ (1, 3, 10)\n",
        "def evaluate(test_graph, model, test_triplets, num_entity, hits=[], eval_bz=100):\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # During the evaluation we work with the entire graph\n",
        "        \n",
        "        # embedding and relations without gradients\n",
        "        embedding, w = model.evaluate(test_graph)\n",
        "        s = test_triplets[:, 0]\n",
        "        r = test_triplets[:, 1]\n",
        "        o = test_triplets[:, 2]\n",
        "\n",
        "        # perturb subject\n",
        "        ranks_s = perturb_and_get_rank(embedding, w, o, r, s, num_entity, eval_bz)\n",
        "        # perturb object\n",
        "        ranks_o = perturb_and_get_rank(embedding, w, s, r, o, num_entity, eval_bz)\n",
        "\n",
        "        ranks = torch.cat([ranks_s, ranks_o])\n",
        "        ranks += 1 # change to 1-indexed\n",
        "\n",
        "        mrr = torch.mean(1.0 / ranks.float())\n",
        "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
        "\n",
        "        for hit in hits:\n",
        "            avg_count = torch.mean((ranks <= hit).float())\n",
        "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
        "    return mrr.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_11LN9--BVUw",
        "colab_type": "text"
      },
      "source": [
        "# R-GCN Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLwWlaReB8kv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RGCNLayer(nn.Module):\n",
        "  '''\n",
        "  The abstract layer of a R-GCN\n",
        "  '''\n",
        "  \n",
        "  def __init__(self, in_feat, out_feat, bias=None, activation=None,\n",
        "                 self_loop=False, dropout=0.0):\n",
        "    \n",
        "    super(RGCNLayer, self).__init__()\n",
        "        \n",
        "    self.bias = bias\n",
        "    self.activation = activation\n",
        "    self.self_loop = self_loop\n",
        "    \n",
        "    # Bias vector - Size: out_feat\n",
        "    if self.bias == True:\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
        "        nn.init.xavier_uniform_(self.bias,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    # Weight for self loop - Size: in_feat X out_feat\n",
        "    if self.self_loop:\n",
        "        self.loop_weight = nn.Parameter(torch.Tensor(in_feat, out_feat))\n",
        "        nn.init.xavier_uniform_(self.loop_weight,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "    \n",
        "    # Dropout according to the parameter\n",
        "    if dropout:\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    else:\n",
        "        self.dropout = None\n",
        "\n",
        "  # Define how propagation is done in subclass\n",
        "  def propagate(self, g):\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def forward(self, g):\n",
        "      # First of all I consider the self_loop contribution\n",
        "      if self.self_loop:\n",
        "          loop_message = torch.mm(g.ndata['h'], self.loop_weight)\n",
        "          if self.dropout is not None:\n",
        "              loop_message = self.dropout(loop_message)\n",
        "\n",
        "      # The propagation is implemented in each specific RGCN layer\n",
        "      self.propagate(g)\n",
        "\n",
        "      # Apply bias, loop message, and activation\n",
        "      node_repr = g.ndata['h']\n",
        "      if self.bias:\n",
        "          node_repr = node_repr + self.bias\n",
        "      if self.self_loop:\n",
        "          node_repr = node_repr + loop_message\n",
        "      if self.activation:\n",
        "          node_repr = self.activation(node_repr)\n",
        "      \n",
        "      # Output of the layer applying the previous operations\n",
        "      g.ndata['h'] = node_repr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtPxe53-LfWs",
        "colab_type": "text"
      },
      "source": [
        "## Basis Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx7EA7LdLmRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RGCNBasisLayer(RGCNLayer):\n",
        "    \"\"\"\n",
        "    RCNBasisLayer implements the interface of the RGCLayer\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
        "                 activation=None, is_input_layer=False):\n",
        "        \n",
        "        # For the basis layer, the self_loop and dropout have default values,\n",
        "        # respectively False and 0.0\n",
        "        super(RGCNBasisLayer, self).__init__(in_feat, out_feat, bias, activation)\n",
        "        \n",
        "        self.in_feat = in_feat\n",
        "        self.out_feat = out_feat\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.is_input_layer = is_input_layer\n",
        "        \n",
        "        # If the number of features is less than the number of relations, I apply\n",
        "        # the regularization\n",
        "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
        "            self.num_bases = self.num_rels\n",
        "\n",
        "        # Add bases weights\n",
        "        # The weight is a 3-D tensor, because it consider input features, output\n",
        "        # features (matrix) and all possible relationships related to input and\n",
        "        # output features (third dimension)\n",
        "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
        "                                                self.out_feat))\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # Linear combination coefficients\n",
        "            # This is the regularization through basis decomposition\n",
        "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels,\n",
        "                                                    self.num_bases))\n",
        "        \n",
        "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        \n",
        "        if self.num_bases < self.num_rels:\n",
        "            nn.init.xavier_uniform_(self.w_comp,\n",
        "                                    gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def propagate(self, g):\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # Generate all weights from bases\n",
        "            weight = self.weight.view(self.num_bases,\n",
        "                                      self.in_feat * self.out_feat)\n",
        "            weight = torch.matmul(self.w_comp, weight).view(\n",
        "                                    self.num_rels, self.in_feat, self.out_feat)\n",
        "        else:\n",
        "            weight = self.weight\n",
        "\n",
        "        if self.is_input_layer:\n",
        "            def msg_func(edges):\n",
        "                # For input layer, matrix multiply can be converted to be\n",
        "                # An embedding lookup using source node id\n",
        "                embed = weight.view(-1, self.out_feat)\n",
        "                index = edges.data['type'] * self.in_feat + edges.src['id']\n",
        "                return {'msg': embed.index_select(0, index) * edges.data['norm']}\n",
        "        else:\n",
        "            def msg_func(edges):\n",
        "                w = weight.index_select(0, edges.data['type'])\n",
        "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
        "                msg = msg * edges.data['norm']\n",
        "                return {'msg': msg}\n",
        "\n",
        "        g.update_all(msg_func, fn.sum(msg='msg', out='h'), None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JXGXndBLiiE",
        "colab_type": "text"
      },
      "source": [
        " ## Block Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB8ePcXwMP15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RGCNBlockLayer(RGCNLayer):\n",
        "    # in_feat and out_feat correspond to the number of neurons for each block layer\n",
        "    def __init__(self, in_feat, out_feat, num_rels, num_bases, bias=None,\n",
        "                 activation=None, self_loop=False, dropout=0.0):\n",
        "        \n",
        "        super(RGCNBlockLayer, self).__init__(in_feat, out_feat, bias,\n",
        "                                             activation, self_loop=self_loop,\n",
        "                                             dropout=dropout)\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        \n",
        "        assert self.num_bases > 0\n",
        "\n",
        "        self.out_feat = out_feat\n",
        "        self.submat_in = in_feat // self.num_bases\n",
        "        self.submat_out = out_feat // self.num_bases\n",
        "\n",
        "        # Assuming in_feat and out_feat are both divisible by num_bases\n",
        "        # Block diagonal regularization\n",
        "        self.weight = nn.Parameter(torch.Tensor(\n",
        "            self.num_rels, self.num_bases * self.submat_in * self.submat_out))\n",
        "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def msg_func(self, edges):\n",
        "        \n",
        "        # edges.data[type] contains ids of sampled relations. Such ids are used\n",
        "        # as indexes to get the rows from the weight matrix of the block layer\n",
        "        # Before the reshape (.view()) the size of weights is \n",
        "        # (len(edges), self.num_bases * self.submat_in * self.submat_out) \n",
        "        # After the reshape we obtain a 3-dimensional tensor\n",
        "        print('edges.data[type]')\n",
        "        print(edges.data['type'])\n",
        "        print(edges.data['type'].size())\n",
        "        weight = self.weight.index_select(0, edges.data['type'])\n",
        "        weight = weight.view(-1, self.submat_in, self.submat_out)\n",
        "        \n",
        "        # Take the embedding values of source nodes resulting from the sampling process\n",
        "        node = edges.src['h'].view(-1, 1, self.submat_in)\n",
        "        \n",
        "        # Batch matrix matrix multiplication between the embedding values of nodes and the weights\n",
        "        msg = torch.bmm(node, weight).view(-1, self.out_feat)\n",
        "        \n",
        "        return {'msg': msg}\n",
        "\n",
        "    def propagate(self, g):\n",
        "        # The embedding value of each node is update according to the message\n",
        "        # and then is multiplied with the norm value\n",
        "        g.update_all(self.msg_func, fn.sum(msg='msg', out='h'), self.apply_func)\n",
        "\n",
        "    def apply_func(self, nodes):\n",
        "        return {'h': nodes.data['h'] * nodes.data['norm']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIgGxaxSRZRM",
        "colab_type": "text"
      },
      "source": [
        "# R-GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smN7wF3sRgk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseRGCN(nn.Module):\n",
        "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases=-1,\n",
        "                 num_hidden_layers=1, dropout=0, use_cuda=False):\n",
        "        super(BaseRGCN, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.h_dim = h_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        # create rgcn layers\n",
        "        self.build_model()\n",
        "\n",
        "        # create initial features\n",
        "        self.features = self.create_features()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.layers = nn.ModuleList()\n",
        "        # i2h\n",
        "        i2h = self.build_input_layer()\n",
        "        if i2h is not None:\n",
        "            self.layers.append(i2h)\n",
        "        # h2h\n",
        "        for idx in range(self.num_hidden_layers):\n",
        "            h2h = self.build_hidden_layer(idx)\n",
        "            self.layers.append(h2h)\n",
        "        # h2o\n",
        "        h2o = self.build_output_layer()\n",
        "        if h2o is not None:\n",
        "            self.layers.append(h2o)\n",
        "\n",
        "    # initialize feature for each node\n",
        "    def create_features(self):\n",
        "        return None\n",
        "\n",
        "    def build_input_layer(self):\n",
        "        return None\n",
        "\n",
        "    def build_hidden_layer(self, idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build_output_layer(self):\n",
        "        return None\n",
        "\n",
        "    def forward(self, g):\n",
        "        # The first step of the forward returns graph of the data\n",
        "        # Data related to the nodes include ids (no features for nodes) and norms\n",
        "        if self.features is not None:\n",
        "            g.ndata['id'] = self.features\n",
        "        for layer in self.layers:\n",
        "            layer(g)\n",
        "        return g.ndata.pop('h')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWvu7NIgS5OE",
        "colab_type": "text"
      },
      "source": [
        "# Link prediction stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2ciIAQFS8Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from dgl.contrib.data import load_data\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, num_nodes, h_dim):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        # Create an embedding value [0,1] for each node of the graph\n",
        "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
        "\n",
        "    def forward(self, g):\n",
        "        # Retrieve all nodes from the graph in the training data.\n",
        "        # Consider only the nodes resultin from the sampling process\n",
        "        node_id = g.ndata['id'].squeeze()\n",
        "        # Assign a specific embedding to each of this node\n",
        "        g.ndata['h'] = self.embedding(node_id)\n",
        "\n",
        "class RGCN(BaseRGCN):\n",
        "    def build_input_layer(self):\n",
        "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
        "\n",
        "    def build_hidden_layer(self, idx):\n",
        "        # Build a number of hidden layer according to the parameter\n",
        "        # Add a relu activation function, until I create the last layer\n",
        "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
        "        \n",
        "        return RGCNBlockLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
        "                         activation=act, self_loop=True, dropout=self.dropout)\n",
        "\n",
        "class LinkPredict(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
        "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
        "        super(LinkPredict, self).__init__()\n",
        "        \n",
        "        # Build RGCN Layers\n",
        "        # Num rels is doubled, because we consider both directions\n",
        "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
        "                         num_hidden_layers, dropout, use_cuda)\n",
        "        \n",
        "        # Define regularization\n",
        "        self.reg_param = reg_param\n",
        "        \n",
        "        # Define relations and normalize them\n",
        "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
        "        nn.init.xavier_uniform_(self.w_relation,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def calc_score(self, embedding, triplets):\n",
        "        # Apply DistMult for scoring\n",
        "        # embedding contains the embedding values of the node after the propagation\n",
        "        # within the RGCN Block layer\n",
        "        # triplets contains all triples resulting from the negative sampling process\n",
        "        s = embedding[triplets[:,0]]\n",
        "        r = self.w_relation[triplets[:,1]]\n",
        "        o = embedding[triplets[:,2]]\n",
        "        score = torch.sum(s * r * o, dim=1)\n",
        "        return score\n",
        "\n",
        "    def forward(self, g):\n",
        "        return self.rgcn.forward(g)\n",
        "\n",
        "    def evaluate(self, g):\n",
        "        # Get embedding and relation weight without grad\n",
        "        embedding = self.forward(g)\n",
        "        return embedding, self.w_relation\n",
        "\n",
        "    def regularization_loss(self, embedding):\n",
        "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
        "\n",
        "    def get_loss(self, g, triplets, labels):\n",
        "        # Triplets is a list of data samples (positive and negative),\n",
        "        # because we train the network using negative sampling\n",
        "        # Each row in the triplets is a 3-tuple of (source, relation, destination)\n",
        "        embedding = self.forward(g)        \n",
        "        \n",
        "        # The score is computed with the value-by-value multiplication of\n",
        "        # the embedding values of data produced by the negative sampling process\n",
        "        # and sum them using the vertical dimension\n",
        "        score = self.calc_score(embedding, triplets)\n",
        "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
        "        reg_loss = self.regularization_loss(embedding)\n",
        "        \n",
        "        return predict_loss + self.reg_param * reg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWh7JG1YZRqW",
        "colab_type": "text"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF02YEYKFQT5",
        "colab_type": "text"
      },
      "source": [
        "Default values:\n",
        "\n",
        "\n",
        "```\n",
        "args = {'dropout': 0.2,\n",
        "        'n_hidden': 500,\n",
        "        'gpu': 1,\n",
        "        'lr': 1e-2,\n",
        "        'n_bases': 100,\n",
        "        'n_layers': 2,\n",
        "        'n_epochs': 6000,\n",
        "        'dataset': 'FB15k-237',\n",
        "        'eval_batch_size': 500,\n",
        "        'regularization': 0.01,\n",
        "        'grad_norm': 1.0,\n",
        "        'graph_batch_size': 30000,\n",
        "        'graph_split_size': 0.5,\n",
        "        'negative_sample': 10,\n",
        "        'evaluate_every': 500}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfuN9kevZ7Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {'dropout': 0.2,\n",
        "        'n_hidden': 500,\n",
        "        'gpu': 1,\n",
        "        'lr': 1e-2,\n",
        "        'n_bases': 100,\n",
        "        'n_layers': 2,\n",
        "        'n_epochs': 10,\n",
        "        'dataset': 'FB15k-237',\n",
        "        'eval_batch_size': 500,\n",
        "        'regularization': 0.01,\n",
        "        'grad_norm': 1.0,\n",
        "        'graph_batch_size': 3000,\n",
        "        'graph_split_size': 0.5,\n",
        "        'negative_sample': 10,\n",
        "        'evaluate_every': 10}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi7zJ-75Tme_",
        "colab_type": "text"
      },
      "source": [
        "## Scripts for running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH9q4lKuTojg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load graph data\n",
        "data = load_data(args['dataset'])\n",
        "num_nodes = data.num_nodes\n",
        "train_data = data.train\n",
        "valid_data = data.valid\n",
        "test_data = data.test\n",
        "num_rels = data.num_rels\n",
        "\n",
        "# check cuda\n",
        "use_cuda = args['gpu'] >= 0 and torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    torch.device('cuda')\n",
        "\n",
        "# build test graph\n",
        "test_graph, test_rel, test_norm = build_test_graph(\n",
        "    num_nodes, num_rels, train_data)\n",
        "test_deg = test_graph.in_degrees(\n",
        "            range(test_graph.number_of_nodes())).float().view(-1,1)\n",
        "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
        "test_rel = torch.from_numpy(test_rel).view(-1, 1)\n",
        "# test_rel = torch.from_numpy(test_rel)\n",
        "test_norm = torch.from_numpy(test_norm).view(-1, 1)\n",
        "test_graph.ndata.update({'id': test_node_id, 'norm': test_norm})\n",
        "test_graph.edata['type'] = test_rel    \n",
        "    \n",
        "# create model\n",
        "model = LinkPredict(num_nodes,\n",
        "                    args['n_hidden'],\n",
        "                    num_rels,\n",
        "                    num_bases=args['n_bases'],\n",
        "                    num_hidden_layers=args['n_layers'],\n",
        "                    dropout=args['dropout'],\n",
        "                    use_cuda=use_cuda,\n",
        "                    reg_param=args['regularization'])\n",
        "\n",
        "# validation and testing triplets\n",
        "valid_data = torch.LongTensor(valid_data)\n",
        "test_data = torch.LongTensor(test_data)\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "# build adj list and calculate degrees for sampling\n",
        "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "model_state_file = 'model_state.pth'\n",
        "forward_time = []\n",
        "backward_time = []\n",
        "loss_values = []\n",
        "train_start = time.time()\n",
        "\n",
        "# Training loop\n",
        "print(\"start training...\")\n",
        "\n",
        "epoch = 0\n",
        "best_mrr = 0\n",
        "while True:\n",
        "    model.train()\n",
        "    epoch += 1\n",
        "\n",
        "    # Perform edge neighborhood sampling to generate training graph and data\n",
        "    # The training stage is performed on a sample graph (not the entire graph)\n",
        "    g, node_id, edge_type, node_norm, data, labels = \\\n",
        "        generate_sampled_graph_and_labels(\n",
        "            train_data, \n",
        "            args['graph_batch_size'], # Useful for sampling\n",
        "            args['graph_split_size'], # Useful for sampling\n",
        "            num_rels, \n",
        "            adj_list,\n",
        "            degrees,\n",
        "            args['negative_sample'])\n",
        "    \n",
        "    print(\"Done edge sampling\")\n",
        "\n",
        "    # Set node/edge feature\n",
        "    # Reminder of returning values of the generate_sampled_graph_and_labels func\n",
        "    # g -- DGL graph\n",
        "    # node_id -- (vector) ids of unique nodes (result of the sampling process)\n",
        "    # edge_type -- (vector) ids of relations (result of the sampling process)\n",
        "    # node_norm -- (vector) normalized degree values of each node (same dim of node_id)\n",
        "    # data -- (matrix) triple samples considering positive and negative samples\n",
        "    # labels -- (vector) size(, positive + negative samples)\n",
        "    \n",
        "    node_id = torch.from_numpy(node_id).view(-1, 1)\n",
        "    edge_type = torch.from_numpy(edge_type)\n",
        "    node_norm = torch.from_numpy(node_norm).view(-1, 1)\n",
        "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
        "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1) # same dim of node_id\n",
        "    \n",
        "    # Use cuda to store the tensors\n",
        "    if use_cuda:\n",
        "        node_id, deg = node_id.cuda(), deg.cuda()\n",
        "        edge_type, node_norm = edge_type.cuda(), node_norm.cuda()\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "    \n",
        "    # The DGL graph is generated, we need to bind the data on the graph elements\n",
        "    g.ndata.update({'id': node_id, 'norm': node_norm})    \n",
        "    g.edata['type'] = edge_type\n",
        "\n",
        "    t0 = time.time()\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = model.get_loss(g, data, labels)\n",
        "    t1 = time.time()\n",
        "    \n",
        "    # Compute back propagation\n",
        "    loss.backward()\n",
        "    \n",
        "    # Clip gradients before optimizing to avoid gradients explosion problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), args['grad_norm'])\n",
        "    optimizer.step()\n",
        "    t2 = time.time()\n",
        "\n",
        "    forward_time.append(t1 - t0)\n",
        "    backward_time.append(t2 - t1)\n",
        "    loss_values.append(loss.item())\n",
        "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n",
        "          format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform the evaluation of the model when the epoch is multiple of evaluate_every\n",
        "    if epoch % args['evaluate_every'] == 0:\n",
        "        \n",
        "        # Perform validation on CPU because full graph is too large\n",
        "        if use_cuda:\n",
        "            model.cpu()\n",
        "        model.eval()\n",
        "        \n",
        "        print(\"start eval\")\n",
        "        mrr = evaluate(test_graph, model, valid_data, num_nodes,\n",
        "                             hits=[1, 3, 10], eval_bz=args['eval_batch_size'])\n",
        "        \n",
        "        # save best model\n",
        "        if mrr < best_mrr:\n",
        "            if epoch >= args['n_epochs']:\n",
        "                break\n",
        "        else:\n",
        "            best_mrr = mrr\n",
        "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
        "                       model_state_file)\n",
        "        if use_cuda:\n",
        "            model.cuda()\n",
        "\n",
        "print(\"training done\")\n",
        "train_done = time.time()\n",
        "print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n",
        "print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))\n",
        "print(\"Training time: {:4f}min\").format((train_done - train_start)/60)\n",
        "\n",
        "print(\"\\nstart testing:\")\n",
        "# use best model checkpoint\n",
        "checkpoint = torch.load(model_state_file)\n",
        "if use_cuda:\n",
        "    model.cpu() # test on CPU\n",
        "model.eval()\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "print(\"Using best epoch: {}\".format(checkpoint['epoch']))\n",
        "evaluate(test_graph, model, test_data, num_nodes, hits=[1, 3, 10],\n",
        "               eval_bz=args['eval_batch_size'])\n",
        "\n",
        "# Print loss values\n",
        "plt.plot(loss_values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}